{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 读写文件\n",
    "加载和保存张量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')\n",
    "\n",
    "x2 = torch.load('x-file')\n",
    "x2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "存储一个张量列表，然后把它们读回内存"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "写入或读取从字符串映射到张量的字典"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 加载和保存模型参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('hidden.weight',\n              tensor([[ 0.1328, -0.0106,  0.0247,  ..., -0.0915, -0.0072, -0.1126],\n                      [ 0.0513,  0.1855, -0.0187,  ..., -0.1696,  0.0569, -0.0919],\n                      [-0.1628, -0.0544,  0.0200,  ..., -0.2151, -0.1535,  0.1455],\n                      ...,\n                      [ 0.1156, -0.0784,  0.0283,  ...,  0.1045, -0.0789, -0.0448],\n                      [ 0.1106,  0.1883,  0.0755,  ...,  0.1454, -0.0547,  0.0359],\n                      [ 0.2050,  0.1042, -0.1148,  ...,  0.1685, -0.0512,  0.0396]])),\n             ('hidden.bias',\n              tensor([-0.1594,  0.1560, -0.2223, -0.1120,  0.1184,  0.1972, -0.0382, -0.1765,\n                       0.0278, -0.1608, -0.0665, -0.1513,  0.1445,  0.1863,  0.1465, -0.1201,\n                      -0.0047,  0.0922, -0.1336, -0.0307,  0.0591, -0.2057, -0.0770, -0.1135,\n                      -0.1482, -0.1768, -0.1031, -0.0599, -0.1205,  0.1301,  0.2203, -0.0973,\n                      -0.0070,  0.2058,  0.0844, -0.1665,  0.1732, -0.1484, -0.1436, -0.1880,\n                      -0.1330,  0.0583,  0.1821, -0.0965,  0.2144,  0.2075,  0.1032, -0.1895,\n                       0.0690,  0.1167, -0.2186, -0.0007,  0.0855,  0.0491, -0.0134,  0.1187,\n                      -0.1334,  0.0024,  0.0869, -0.0472, -0.0134,  0.0042, -0.0781, -0.1838,\n                      -0.1665,  0.0815,  0.0356, -0.0775, -0.0947,  0.0242, -0.0252,  0.1791,\n                      -0.2211,  0.1212, -0.1429, -0.0551,  0.1480, -0.1126, -0.0160, -0.1323,\n                       0.2007, -0.0766, -0.2127,  0.0909,  0.0103,  0.1899,  0.1645,  0.0555,\n                       0.1390, -0.0816,  0.2175,  0.1475, -0.1731,  0.1205,  0.2120,  0.1368,\n                       0.0331, -0.1646, -0.1549, -0.1299, -0.0529, -0.0223, -0.0615,  0.1945,\n                      -0.2159, -0.0132,  0.2169, -0.1739, -0.0268, -0.0249, -0.0480, -0.2088,\n                       0.1083,  0.1161, -0.0684, -0.1009,  0.1669,  0.0929,  0.0874,  0.1430,\n                      -0.2192, -0.1834, -0.1621, -0.1986,  0.0497,  0.0897,  0.0035,  0.0107,\n                       0.1081, -0.0596, -0.2093,  0.0680,  0.1781, -0.0413,  0.1216,  0.1430,\n                      -0.0150, -0.1572, -0.1092, -0.1301, -0.1876, -0.1424, -0.1649,  0.0880,\n                       0.1102, -0.0008, -0.0932, -0.1069,  0.1487,  0.1383,  0.1874, -0.1814,\n                       0.1760, -0.0536, -0.1031, -0.0033, -0.1447, -0.1489, -0.0913,  0.1421,\n                       0.0509,  0.0638, -0.2174,  0.1213,  0.0523, -0.0497,  0.0529,  0.0138,\n                      -0.0137, -0.2142, -0.0132, -0.2168,  0.0502, -0.0750,  0.1141, -0.0777,\n                       0.0261, -0.0297,  0.1177, -0.0759,  0.0921, -0.0092,  0.0180,  0.0393,\n                       0.0906,  0.0144,  0.0247,  0.1114, -0.0468,  0.1287, -0.1619, -0.1406,\n                      -0.0739,  0.1722, -0.0173,  0.0595,  0.0868, -0.1101, -0.0267,  0.1114,\n                       0.0256, -0.1714,  0.2199,  0.0183, -0.0191,  0.1863,  0.0474, -0.1729,\n                       0.1395,  0.1717,  0.0798, -0.0294, -0.0749,  0.0965, -0.0840,  0.1816,\n                      -0.0068,  0.0531,  0.1273,  0.0536,  0.1618, -0.1889, -0.0539,  0.1048,\n                       0.0077, -0.1173,  0.2032,  0.0042,  0.0811,  0.0467,  0.2003,  0.2068,\n                       0.1682, -0.0970, -0.0678,  0.1339,  0.1996, -0.1056, -0.0343,  0.1229,\n                      -0.1270, -0.0997,  0.0860, -0.1448,  0.1737, -0.1138,  0.0258, -0.1003,\n                       0.1669,  0.1173, -0.0524, -0.0135,  0.1468,  0.1582, -0.1385,  0.0161])),\n             ('output.weight',\n              tensor([[-0.0169,  0.0624, -0.0623,  ...,  0.0229,  0.0466,  0.0440],\n                      [-0.0317,  0.0350, -0.0100,  ...,  0.0010, -0.0108, -0.0526],\n                      [-0.0510, -0.0215,  0.0595,  ...,  0.0518, -0.0273,  0.0309],\n                      ...,\n                      [-0.0269,  0.0458, -0.0029,  ...,  0.0034,  0.0178, -0.0230],\n                      [ 0.0329, -0.0109, -0.0239,  ...,  0.0557,  0.0293, -0.0595],\n                      [ 0.0175,  0.0012, -0.0551,  ..., -0.0264,  0.0346, -0.0209]])),\n             ('output.bias',\n              tensor([ 0.0180, -0.0318, -0.0007, -0.0030, -0.0443,  0.0111,  0.0528, -0.0313,\n                      -0.0562, -0.0362]))])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将模型的参数存储在一个叫做“mlp.params”的文件中"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "实例化了原始多层感知机模型的一个备份。 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "MLP(\n  (hidden): Linear(in_features=20, out_features=256, bias=True)\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[True, True, True, True, True, True, True, True, True, True],\n        [True, True, True, True, True, True, True, True, True, True]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}